{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "# Check python version, this should match with the current python version on Spark nodes. \n",
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Check pyspark location\n",
    "import pyspark\n",
    "print(pyspark.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the packages to include\n",
    "\n",
    "# For delta compatibility with spark, check https://docs.delta.io/latest/releases.html\n",
    "packages = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",  # Hadoop-AWS package,\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\",  # Kafka package\n",
    "    \"za.co.absa:abris_2.12:6.4.1\", # ABRiS package, used for integration with Confluent Schema Registry,for more information please refer to https://github.com/AbsaOSS/ABRiS,\n",
    "    \"org.apache.spark:spark-avro_2.12:3.5.1\",\n",
    "    \"io.delta:delta-spark_2.12:3.2.0\" # Delta package\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinIO endpoint and credentials\n",
    "minio_endpoint = \"http://minio:9000\"\n",
    "minio_access_key = \"ouxhmt4Ez73Rt8HSNQFu\"\n",
    "minio_secret_key = \"nkEwOH7TsENA7hRSkH0VSRMqdNq65ystE2BODdHW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "conf = (\n",
    "            SparkConf()\n",
    "            #Set name for the application\n",
    "            .setAppName(\"test_spark_kafka_confluent_schema_avro_connection\")\n",
    "            # Set packages to be included in application. Note that packges dependencies will also be installed\n",
    "            .set(\"spark.jars.packages\", \",\".join(packages))\n",
    "            # For more information, please refer to https://github.com/AbsaOSS/ABRiS/issues/355\n",
    "            #Include this dependencies over conflicted spark-avro_2.12 in za.co.absa:abris_2.12:6.4.1 package\n",
    "            # Set additional repositories to search for packages and its transitional dependencies.\n",
    "            # For more information, check https://spark.apache.org/docs/latest/configuration.html#runtime-environment\n",
    "            # For search order of repos, check https://stackoverflow.com/a/51435038\n",
    "            .set(\"spark.jars.repositories\", \n",
    "                \"https://packages.confluent.io/maven\") \n",
    "            # Set sql extensions and catalog for Delta format.  For more information, please refer to https://spark.apache.org/docs/latest/configuration.html#static-sql-configuration\n",
    "            .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "            .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "            ## For more information on spark integration with minio, please refer to https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html\n",
    "            # Set configurations for authorization to minio.\n",
    "            # For more information on minio access keys, please refer to https://min.io/docs/minio/linux/administration/identity-access-management/minio-user-management.html#id3            .set(\"spark.hadoop.fs.s3a.access.key\", minio_access_key)\n",
    "            .set(\"spark.hadoop.fs.s3a.access.key\", minio_access_key)\n",
    "            .set(\"spark.hadoop.fs.s3a.secret.key\", minio_secret_key)\n",
    "            .set(\"spark.hadoop.fs.s3a.endpoint\", minio_endpoint)\n",
    "            # Set configuration to enable SSL connection\n",
    "            .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "            # Set config for interacting with MinIO object storage\n",
    "            .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "            .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "            # Set default connection configuration for connection to MinIO\n",
    "            .set(\"spark.hadoop.fs.s3a.attempts.maximum\", \"1\")\n",
    "            .set(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "            .set(\"spark.hadoop.fs.s3a.connection.timeout\", \"10000\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://packages.confluent.io/maven added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /config/.ivy2/cache\n",
      "The jars for the packages stored in: /config/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "za.co.absa#abris_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1a9c8ebf-2bff-47a4-aa5d-36a5416222f5;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound za.co.absa#abris_2.12;6.4.1 in central\n",
      "\tfound io.confluent#kafka-avro-serializer;6.2.1 in repo-1\n",
      "\tfound org.apache.avro#avro;1.10.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.10.5.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.10.5 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound io.confluent#kafka-schema-serializer;6.2.1 in repo-1\n",
      "\tfound io.confluent#kafka-schema-registry-client;6.2.1 in repo-1\n",
      "\tfound jakarta.ws.rs#jakarta.ws.rs-api;2.1.6 in central\n",
      "\tfound org.glassfish.jersey.core#jersey-common;2.34 in central\n",
      "\tfound jakarta.annotation#jakarta.annotation-api;1.3.5 in central\n",
      "\tfound org.glassfish.hk2.external#jakarta.inject;2.6.1 in central\n",
      "\tfound org.glassfish.hk2#osgi-resource-locator;1.0.3 in central\n",
      "\tfound io.swagger#swagger-annotations;1.6.2 in central\n",
      "\tfound io.swagger#swagger-core;1.6.2 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.2.1 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-yaml;2.10.5 in central\n",
      "\tfound org.yaml#snakeyaml;1.26 in central\n",
      "\tfound io.swagger#swagger-models;1.6.2 in central\n",
      "\tfound com.google.guava#guava;30.1.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.8.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound io.confluent#common-utils;6.2.1 in repo-1\n",
      "\tfound za.co.absa.commons#commons_2.12;1.0.0 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.1 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 1103ms :: artifacts dl 65ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.10.5.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-yaml;2.10.5 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;30.1.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.confluent#common-utils;6.2.1 from repo-1 in [default]\n",
      "\tio.confluent#kafka-avro-serializer;6.2.1 from repo-1 in [default]\n",
      "\tio.confluent#kafka-schema-registry-client;6.2.1 from repo-1 in [default]\n",
      "\tio.confluent#kafka-schema-serializer;6.2.1 from repo-1 in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\tio.swagger#swagger-annotations;1.6.2 from central in [default]\n",
      "\tio.swagger#swagger-core;1.6.2 from central in [default]\n",
      "\tio.swagger#swagger-models;1.6.2 from central in [default]\n",
      "\tjakarta.annotation#jakarta.annotation-api;1.3.5 from central in [default]\n",
      "\tjakarta.ws.rs#jakarta.ws.rs-api;2.1.6 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.avro#avro;1.10.2 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.2.1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.8.0 from central in [default]\n",
      "\torg.glassfish.hk2#osgi-resource-locator;1.0.3 from central in [default]\n",
      "\torg.glassfish.hk2.external#jakarta.inject;2.6.1 from central in [default]\n",
      "\torg.glassfish.jersey.core#jersey-common;2.34 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\torg.yaml#snakeyaml;1.26 from central in [default]\n",
      "\tza.co.absa#abris_2.12;6.4.1 from central in [default]\n",
      "\tza.co.absa.commons#commons_2.12;1.0.0 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.4 by [org.apache.spark#spark-avro_2.12;3.5.1] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   50  |   0   |   0   |   3   ||   47  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1a9c8ebf-2bff-47a4-aa5d-36a5416222f5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 47 already retrieved (0kB/17ms)\n",
      "24/07/16 09:28:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Initialize SparkSession\n",
    "\n",
    "spark =SparkSession.builder.config(conf=conf).master(\"spark://spark-master:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.column import Column, _to_java_column\n",
    "\n",
    "def from_avro(col, config):\n",
    "    \"\"\"\n",
    "    avro deserialize\n",
    "\n",
    "    :param col (PySpark column / str): column name \"key\" or \"value\"\n",
    "    :param config (za.co.absa.abris.config.FromAvroConfig): abris config, generated from abris_config helper function\n",
    "    :return: PySpark Column\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    abris_avro = jvm_gateway.za.co.absa.abris.avro\n",
    "\n",
    "    return Column(abris_avro.functions.from_avro(_to_java_column(col), config))\n",
    "\n",
    "def from_avro_abris_config(config_map, topic, is_key):\n",
    "    \"\"\"\n",
    "    Create from avro abris config with a schema url\n",
    "\n",
    "    :param config_map (dict[str, str]): configuration map to pass to deserializer, ex: {'schema.registry.url': 'http://localhost:8081'}\n",
    "    :param topic (str): kafka topic\n",
    "    :param is_key (bool): boolean\n",
    "    :return: za.co.absa.abris.config.FromAvroConfig\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    scala_map = jvm_gateway.PythonUtils.toScalaMap(config_map)\n",
    "\n",
    "    return jvm_gateway.za.co.absa.abris.config \\\n",
    "        .AbrisConfig \\\n",
    "        .fromConfluentAvro() \\\n",
    "        .downloadReaderSchemaByLatestVersion() \\\n",
    "        .andTopicNameStrategy(topic, is_key) \\\n",
    "        .usingSchemaRegistry(scala_map)\n",
    "\n",
    "def to_avro(col, config):\n",
    "    \"\"\"\n",
    "    avro serialize\n",
    "    :param col (PySpark column / str): column name \"key\" or \"value\"\n",
    "    :param config (za.co.absa.abris.config.ToAvroConfig): abris config, generated from abris_config helper function\n",
    "    :return: PySpark Column\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    abris_avro = jvm_gateway.za.co.absa.abris.avro\n",
    "\n",
    "    return Column(abris_avro.functions.to_avro(_to_java_column(col), config))\n",
    "\n",
    "def to_avro_abris_config(config_map, topic, is_key):\n",
    "    \"\"\"\n",
    "    Create to avro abris config with a schema url\n",
    "\n",
    "    :param config_map (dict[str, str]): configuration map to pass to the serializer, ex: {'schema.registry.url': 'http://localhost:8081'}\n",
    "    :param topic (str): kafka topic\n",
    "    :param is_key (bool): boolean\n",
    "    :return: za.co.absa.abris.config.ToAvroConfig\n",
    "    \"\"\"\n",
    "    jvm_gateway = SparkContext._active_spark_context._gateway.jvm\n",
    "    scala_map = jvm_gateway.PythonUtils.toScalaMap(config_map)\n",
    "\n",
    "    return jvm_gateway.za.co.absa.abris.config \\\n",
    "        .AbrisConfig \\\n",
    "        .toConfluentAvro() \\\n",
    "        .downloadSchemaByLatestVersion() \\\n",
    "        .andTopicNameStrategy(topic, is_key) \\\n",
    "        .usingSchemaRegistry(scala_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to topic of a table with messages produced by Debezium.\n",
    "kafka_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-broker-0:9092,kafka-broker-1:9092\") \\\n",
    "  .option(\"subscribe\", \"postgresql.devschema.person\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Set from avro config setting with endpoint, topic, key value as value\n",
    "from_avro_abris_settings_key_false = from_avro_abris_config({'schema.registry.url': 'http://kafka-schema-registry:8081'}, 'postgresql.devschema.person', False)\n",
    "\n",
    "#Set from avro config setting with endpoint, topic, key value as key\n",
    "from_avro_abris_settings_key_true = from_avro_abris_config({'schema.registry.url': 'http://kafka-schema-registry:8081'}, 'postgresql.devschema.person', True)\n",
    "\n",
    "# Parse value with avro format\n",
    "kafka_df_avro_key_value_converted = kafka_df.withColumn(\"value_parsed\", from_avro(\"value\", from_avro_abris_settings_key_false)).withColumn(\"key_parsed\", from_avro(\"key\", from_avro_abris_settings_key_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 09:28:48 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7c6bf588-987e-4939-9c98-5d1045b0aa4b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/07/16 09:28:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/07/16 09:28:49 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "## Use this to test connection to memory sink\n",
    "# Query to memory as name postgresql_devschema_persion\n",
    "# For output sink memory, please refer to https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks\n",
    "query = kafka_df_avro_key_value_converted \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"postgresql_devschema_person_test\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------+-----------+\n",
      "|namespace|tableName                       |isTemporary|\n",
      "+---------+--------------------------------+-----------+\n",
      "|         |postgresql_devschema_person_test|false      |\n",
      "+---------+--------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show if memory view table is created by listing tables\n",
    "spark.sql('show tables').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|key_parsed|value_parsed                                                                                                                                                                                                                                                      |\n",
      "+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{1}       |{NULL, {1, Alice, 30}, {2.7.0.Final, postgresql, postgresql, 1721036562977, first, devdatabase, [null,\"31232792\"], 1721036562977778, 1721036562977778000, devschema, person, 756, 31232792, NULL}, NULL, r, 1721036563118, 1721036563118103, 1721036563118103221} |\n",
      "|{2}       |{NULL, {2, Bob, 25}, {2.7.0.Final, postgresql, postgresql, 1721036562977, true, devdatabase, [null,\"31232792\"], 1721036562977778, 1721036562977778000, devschema, person, 756, 31232792, NULL}, NULL, r, 1721036563120, 1721036563120655, 1721036563120655190}    |\n",
      "|{3}       |{NULL, {3, Charlie, 35}, {2.7.0.Final, postgresql, postgresql, 1721036562977, last, devdatabase, [null,\"31232792\"], 1721036562977778, 1721036562977778000, devschema, person, 756, 31232792, NULL}, NULL, r, 1721036563120, 1721036563120861, 1721036563120861545}|\n",
      "+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select from that created memory table\n",
    "spark.sql('select key_parsed,value_parsed from postgresql_devschema_person_test').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 09:31:03 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 09:31:04 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "## Use this to test connection to Delta lake table format sink \n",
    "# Query to file in MinIO with path s3a://<bucket>/path/to/your/folder\n",
    "# For output sink file, please refer to https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks\n",
    "query = (kafka_df_avro_key_value_converted \n",
    "    .writeStream \n",
    "    # Output format (can be \"delta\", \"parquet\", \"json\", \"csv\", etc.) \n",
    "    .format(\"delta\") \n",
    "    # Output directory\n",
    "    .option(\"path\", \"s3a://test2/data\")  \n",
    "    # Checkpoint location for recovery\n",
    "    .option(\"checkpointLocation\", \"s3a://test2/checkpoint\") \n",
    "    # Output mode: append, complete, update\n",
    "    .outputMode(\"append\") \n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
